---
layout: post   
author: 'Rahul Venugopal'    
title: "Lecture 10: Hamming Codes and Cryptography"   
mathjax: true
---


# Hamming Codes

### Introduction and motivation behind Hamming codes

Hamming was writing programs and there were programming errors. This was counter productive since these bugs were caught only after a long execution time. The motivation came from such experience to have codes which can detect and correct errors. Bell labs was a perfect place for innovations with people like Tukey (who came up with FFT), Bode etc. inhabiting the place at same time.

Hamming proposed [7,4] codes. Here the rate, $R = 4/7 = 0.5714$
If we compare this against the rate of repetition code R3 ($rate = 1/3 = 0.333$) it is better
Both of these correct only $1 bit$ error
Hamming distance, d = 3 for both the coding schemes

**Is it fair to make the statement that Hamming codes are better than R3 code?**
The block size is different in Hamming code and R3. Hamming code takes in 4 bits and output 7 bits, where as R3 code takes in 4 bits and output 12 bits
The 1 bit correction is happening over the block size. So, comparison of different schemes of code is not straight forward

Generally, the comparison is made using simulations of transmission over binary symmetric channel (BSC) and $iid$ source. One more common method is by computing error rates from analytical expressions

Hamming code is a fancy way of parity check. There are three parity bits.
We have total n bits of which k data bits are there. This means that $n-k$ parity bits are there
If we have to correct all $1 bit$ errors, how do we find $n$ and $k$

There are $n$ ways to have 1 bit errors and only $1$ way to have no errors
The $n-k$ parity bits should capture $n+1$ possible events

We will have $2^{n-k}$ states for the parity bits

So, for the parity bits to catch all one bit errors and no errors, it should satisfy the criteria

​														$2^{n-k} >= n+1$

|        k         |  n   | Rate |
| :--------------: | :--: | :--: |
| $2^{n-1} >= n+1$ |  3   | 1/3  |
| $2^{n-2} >= n+1$ |  5   | 2/5  |
| $2^{n-3} >= n+1$ |  6   | 3/6  |
| $2^{n-4} >= n+1$ |  7   | 4/7  |

What would a plot of $rate, k/n$ vs $n$ look like?

### Hamming code in action

![image-20200415010334731](../images/Lec10/HammingCode.png)

Bits 3,5,6,7 are data bits are known. We are going from left to right when we lay out the data bits. The mathematical operation to get parity bit is **add and perform modulo 2 operation**

We could have padded the parity bits, k after 4 data bits also. But the trick/beauty of hamming codes is that we would be able to know the position/location of the error also

Let us demonstrate using an example:
The message transmitted is **1100110** and the message received is **1110110**

In the example, fifth bit is in error (only one bit error)
Now, we have to perform row wise parity check. If the computed parity is in agreement with the parity bit, we get a **zero**, if the calculated parity is not matching the parity bit, we get a **one** indicating wrong

We get 101 after this operation, which implies that fifth bit is in error. By checking the parity equations, we are able to locate the error (this error can be in data bits or parity bits)

We can make the fourth bit into 1 and follow the same procedure to get the parity equations pointing fourth bit location (100). The code is read bottom up

If we look at the table column wise, the number of ones tell us that all those rows should be wrong for that bit to be an error bit
For 7th column, all three parity equations has to be wrong. For column number 2, only middle row need to be wrong. So on and so forth. This is a true elegance of Hamming codes

![image-20200415010427689](../images/Lec10/HammingCodeDecode.png)

In Hamming codes, the information and parity bits are mixed up. So reading the information is not straight forward compared to code schemes where parity bits are padded to information bits

#### The minimum distance of Hamming [7,4] code is 3

There are always $2^{k}$ codewords for $(n,k)$ hamming code
When we change one consecutive bit in input, we change the hamming distance by either 3 or 4

#### Hamming codes exist for $[2^m-1, 2^m-m-1]$ for all integers m>2

After [7,4] the next is [15,10]. Hamming codes are one bit error correction codes by design

### Performance evaluation of codes

First metric is **rate**. The rate of a code, $R = k/N$ should be closer to 1

Second metric is minimum distance, **d**. Higher the distance, more errors can be corrected. When d is larger, it implies that codewords are very well separated. Here the rate will suffer because $n$ also increases

Third is the probability of error or **BER** (bit error rate). This is computed after correction. In previous example, all the 2 bit or 3 bit errors can’t be corrected
	**BER = Number of bits in error after decoding**/**Total number of bits in the input message**

Fourth is **t**, the total number of errors it can correct in a block

### Analytical derivation of probability of error

For every n bits, what is the probability of error. In $R3$ code, $n=3$ and $k=1 $

Probability of error = $p^3 + 3p^2(1-p)$
p is the probability of error in binary symmetric channel (BSC). There is no memory in the channel

![image-20200415015123209](../images/Lec10/PER_R3.png)

For $R5$ code, the probability of error would be $p^5+5p^4(1-p)+10p^3(1-p)^2$

**Probability of error for Hamming code[7,4] is	 $1$ -[${(1-p)^7+7(1-p)^6p}$]**

# The Art and Science of Cryptography

### Introduction

There are many scenario in real life where we would need to make the message secret either due to trust issues or the possibility of eavesdropping
Examples: Sending money to mother living in a distant place, Keeping secret information (Reference to Dan Brown’s Da Vinci Code), Storing passwords on computer etc.

Passwords cannot be exposed to anyone using the computer. How are passwords stored in a computer?
You don’t store the password at all! Only the hash value of passwords are stored.

Hash functions are one way functions. Hash functions are of the form, $y=f(x)$ and given $x$ we can find $y$, but from $y$ it is very very difficult to find $x$. Going one way is easy, but traversing the other way is difficult. As a generic example consider telephone directory. Say, I give you the name of the person, you can find his/her phone number very easily in less time. If I give you a phone number and you have to find the person, the only way is brute force search of entire directory from page 1 till last page.

Everyone can access the hash value, but finding the inverse is quite difficult (but not impossible)

### Overview

- A brief history of cryptography
- Recent trends in cryptography
- Break the code challenge

### History of Cryptography

In cryptography, the meaning of the message is hidden, but the message itself is not hidden
Historically, secret messages were often hidden. **Steganography** is about hiding messages

There are lots of folklore cryptography in literature. The spy would shave the head and messages are written on the head. Once the hair grows back, spy is ready to travel with the secret message

Edgar Allan Poe used to write steganographic sonnets. Try finding the message from the below sonnet. Here one point to note is there is no encryption only embedding of the message.

![image-20200414202830223](../images/Lec10/EdgarAllenPoe.png)

This is very common in watermarking. The information is not concealed. These are used for authentication. In music composition or digital art, watermarking is commonly used and can be used as a evidence in court of law in establishing ownership

Secret messages can be embedded in simple looking innocent websites or blogs

### Definitions

- **Cryptology** is the study of ciphers
- The message to be encrypted is called **plain text**
- The encrypted message is called **cipher text**
- Code making is called **Cryptography**
- Code breaking is called **Cryptanalysis**
- Chosen plain text attack - You chose the message and you know the cipher text. How do you figure out the algorithm used for encryption

### The Cryptography Problem

![image-20200414203921316](../images/Lec10/ABE.png)

Say, Akbar wants to send some message to Birbal and usually there is some evil minister listening/eavesdropping. The channel through which the communication happens is not secure. So, all the messages are intercepted by the evil minister. How can Akbar transmit message to Birbal without anyone understanding the message? This is the **Cryptography problem**

Another analogy is cricket. Code making can be compared to batting, code breaking is like bowling. There is a constant battle between people who make code and people who break code. Once a weakness is found in the encryption algorithm, that method would never be used. There are standard bodies which authenticate the encryption algorithm just like umpires.

[The code book](https://simonsingh.net/books/the-code-book/) : How to make it,break it, hack it, crack it by Simon Singh and [wikipedia page](https://en.wikipedia.org/wiki/History_of_cryptography) on **History of Cryptography** are two good resources

### Timeline in Cryptography

Roughly we can think of 5 levels

	1. Classical cryptography (3200 BC - AD 400)
 	2. Medieval cryptography (400 Ad - 1800 AD)
 	3. 1800 to World War II cryptography
 	4. World War II cryptography
 	5. Modern cryptography (Mathematical and computing trends after 1949 AD)

Shannon’s paper came out in 1949 which ushered cryptography to new age
![image-20200414205627212](../images/Lec10/ShannonCrypt.png)

Before 1949 there was not much understanding about limits of encryption etc. Many new trends like DNA cryptography, public keys etc. came into being after 1970s.

### Ancient Cryptography

Most ancient civilisations had some forms of cryptography system. For example, some pottery making techniques were encrypted and stored and passed onto same family members only.

In Mahabharatha (800 BC) there are some beautiful examples of cryptography. The wax palace murder attempt and Vidura-Yudhishthira secret talk happening in Mlecchita Vikalpa dialect is an [example](https://drisyadrisya.blogspot.com/2006/01/cryptography-in-mahabharata.html). This is cited as proof of the prevalence of cryptographic methods in ancient India in the [book](https://books.google.co.in/books?id=SEH_rHkgaogC&pg=PA1000&lpg=PA1000&dq=chinese+cryptography+history&source=bl&ots=_2hrl9t0B1&sig=2LAjURo7zlj5YBoExJjZXbjDhNU&hl=en&sa=X&redir_esc=y#v=onepage&q=chinese%20cryptography%20history&f=false) by David Kahn about history of cryptography.

**Kerckhoffs's principle** states that the cipher should be secure even after everything about the system is known except the **key**

Cryptography techniques also improved as medium of communication also improved. From cave drawings, papyrus, pen and paper, computer based digital cryptography, quantum cryptography etc.

### Trivia

Invention of diode and Fast Fourier Transform revolutionised the way large circuits were built to store and transfer communication
Gauss is the actual inventor of idea of FFT
World war accelerated the needs for cryptography techniques (ENIGMA is a good example)
Marian Rejewski laid down foundations for Alan Turing to crack ENIGMA by building bombe
Bombe was an electromechanical computing machine which implemented nonlinear boolean equation solvers
Many pseudo random generators are chaotic but deterministic
Quantum tunnelling or analog circuits are used to generate true random numbers/patterns
