---
layout: post
author: "Harikrishnan N B"
title: "Lecture 6: Source Coding"
---

#### Effiiciency

Efficiency of  a code ($C$) is defined as


$$
 \begin{equation}
    \eta = \frac{H(X)}{L(C)}
\end{equation}
$$
where $H(X)$ is the entropy of the message symbols and $L(C)$ is the average code word length. Also, $0 \leq H(X) \leq L(C)$, this implies $0 \leq \eta \leq 1$.

\begin{enumerate}
    \item Find H(X), L(C) and $\eta$ for the example in Table I (Use Huffman code to find the code word of alphabet):


Table I: Compute the efficiency.

| Alphabet | Probability |
| -------- | ----------- |
| A        | 0.9         |
| B        | 0.1         |

$$
L(C) = 1 \times 0.9 + 1 \times 0.1  = 1 bit\\
H(X) = - 0.9 \log_{2}(0.9) - 0.1 \log_{2}(0.1) = 0.46  bits\\
\eta = \frac{H(X)}{L(C)} = 0.46 = 46 \%\\
$$

Now, how to improve the efficiency of code by still using the huffman code.

This can be done by creating a different alphabet. The different alphabet is obtained by taking two at a time (i.e AA, AB, BA, BB). Here we assume independence, so the probability of A and A is $P(A)\times P(A) = 0.81$. Similarly for AB, BA and BB. Table II shows the new alphabet and its corresponding probabilities.



Table II: Compute the efficiency.

| Alphabet | Probability |
| -------- | ----------- |
| AA       | 0.81        |
| AB       | 0.09        |
| BA       | 0.09        |
| BB       | 0.01        |
