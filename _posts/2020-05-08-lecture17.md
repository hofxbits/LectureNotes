---
layout: post  
author: Abhishek Nandekar  
title: "Random Variables"
type: "Causality"
mathjax: true  
---

## Random Variables

A mapping from $\Omega$ to a space $E$, where often $E=\mathbb R$ or $E=\mathbb R^d$.    
e.g. In the dice experiment, we assign each outcome $f_i$ with the number $X(f_i)=10i$    
$X(f_1) = 10$, $X(f_2) = 20$, $\dots$, $X(f_6)=60$   


In general, we can talk about events generated by $X:\Omega \rightarrow E$ and be able to assign probabilities to them. This is to 'transport' the probabilistic structure to $E$.

$$ P^X(B) = P(X^{-1}(B)) \quad \forall B \subset E$$

where, $X^{-1}(B)$ is the pre-image of $B$  by $X$, i.e., the set of all $w \in \Omega$ such that $X(w) \in B$.


For the probabilities to be assigned, we need $X$ to be a **measureable function**, $X:\Omega \rightarrow E$. Here, $\Omega$ should be the sample space of $(\Omega, \mathcal A, P)$. $X$ should satisfy some mild conditions:

Let a $\tau \in \Omega$ be mapped to $X(\tau)$ by $X$, and let $\\{X\leq x\\}$ represent a subset of $\Omega$ containing all $\tau$ such that $X(\tau) \leq x$
>- **Condition 1:** The set $\\{X\leq x\\}$ is an event for every $x$.
>- __Condition 2:__ The probabilities of the events $\\{X=\infty\\}$ and $\\{X = -\infty\\}$ are **zero**.

These conditions ensure that $X$ is over some space $\Omega$.   


If $X:\Omega \rightarrow \mathbb R$ is defined on $(\Omega,  \mathcal A, P)$, we can ask questions like:    
_"How likely is it that the value of $X$ is equal to 2?"_    
This is the same as $P\\{\tau: X(\tau) = 2\\} = P(X=2) = P_X(2)$. Using these  probabilities, we can get a _probability distribution of $X$._  

**Probability Distribution** forgets about $\Omega$ and records probabilities at different values of $X$.    


**Cumulative Distribution Function (cdf)** of X:     
$$ F_X = P(X\leq x)$$      

The underlying $\Omega$ is a technical device to:
1. Guarantee the existence of random variables
1. To construct random variables
1. Define notions - __correlation, dependence, independence__ based on __joint distribution__ of two or more random variables defined on the same $\Omega$.  


In practice, $\Omega$ is disposed and we work with probability distributions.    

e.g. In the dice experiment, $X(f_i) = 10i$. If the dice is fair, then $F_X$ is given as:
![Chart](../images/CausalityLec1/cdf.png)

e.g. Suppose r.v. $X$ is such that $X(\tau)=a$ $\forall \tau\in\Omega$.  
For $x\geq a$, all $\tau$ have been mapped to $a$ as $X(\tau)=a$ $\forall \tau$        

$$\implies F(x) = P\{X\leq x\} = P(\Omega) = 1 \quad \quad \forall x\geq a$$

For $x<a$, $\\{X\leq x\\}$ is an impossible event because $X(\tau) = a$      
$$ F(x) = P\{X\leq x\} = P(\phi) = 0 \quad\quad \forall x<a$$    

![Chart1](../images/CausalityLec1/a.png)


## Stochastic/Random Processes

A random variable $X$ is a rule for assigning to every outcome $\tau$ of an experiment $S$ a number $X(\tau)$. A __Stochastic Process__ $X(t)$ is a rule for assigning to  every $\tau$ a function $X(t, \tau)$.

Here,  
- __Domain of $\tau$:__ The set of all experimental outcomes
- __Domain of $t$:__ If the domain of $t$ is $\mathbb Z$, $X(t, \tau)$ is known as a _discrete-time process_. If the domain is $\mathbb R$, it is known as a _continuous-time process_.      

Hence, a __stochastic process__ is defined as a _collection/family_ of random variables that is indexed by some mathematical set. Each random variable is __uniquely__ associated with a particular element in the index set.

Index set $\subseteq \mathbb R$, giving a notion of time.  

![Stochastic](../images/CausalityLec1/stochastic.png)

All these random variables are over a common probability space $\Omega(\mathcal A)$, in order to define a joint distribution, correlation etc.   


#### Examples:

1. __Brownian Motion:__ Motion of microscopic particles in collision with other particles.     
$X(t, \tau)$ or $X(t)$ $\rightarrow$ motion of all particles (_ensemble_)         
$X(t, \tau_i)$ $\rightarrow$ motion of a specifice particle (_sample_)    
![Brownian Motion](../images/CausalityLec1/Brownian.png)  

1. Voltage of an ac generator with random amplitude $r$ and phase $\phi$.     
Ensemble $\rightarrow$ family of pure sine waves   
$$X(t) = r\cos(\omega t+\phi) \\ X(t, \tau_i) = r(\tau_i)\cos(\omega t+\phi(\tau_i))$$
![AC](../images/CausalityLec1/AC.png)   


The above processes are stochastic because   

###### Example 1
- Cannot be described in terms of finite number of parameters
- Future of a sample $X(t, \tau_1)$ cannot be determined based on its past
- Statistics of $X(t, \tau)$ can be determined based on a single sample $X(t, \tau_1)$  

###### Example 2
- Completely specified in terms of r.v.'s $r$ and $\phi$.
- If $X(t, \tau_1)$ is known for $t \leq t_0$, then it is determined for $t>t_0$.
- Single sample $X(t, \tau_1)$ does not specify properties of the entire process because it depends only on particular values $r(\tau_1)$ and $\phi(\tau_1)$
